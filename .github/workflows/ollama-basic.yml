name: Ollama AI Processing

on:
  push:
    branches: [ main ]
  workflow_dispatch:
jobs:
  ai-processing:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    - name: System information
      run: |
        echo "Runner OS: ${{ runner.os }}"
        echo "Available disk space:"
        df -h
        echo "Available memory:"
        free -h
        echo "CPU information:"
        nproc
    - name: Install Ollama
      run: |
        curl -fsSL https://ollama.com/install.sh | sh
    - name: Verify Ollama installation
      run: |
        ollama --version
        ollama list
    - name: Cache Ollama models
      uses: actions/cache@v3
      with:
        path: ~/.ollama
        key: ollama-models-${{ runner.os }}-llama3.2-1b
        restore-keys: |
          ollama-models-${{ runner.os }}-
    - name: Check cached models
      run: |
        if [ -d ~/.ollama ]; then
          echo "Model cache exists"
          echo "Cache size: $(du -sh ~/.ollama | cut -f1)"
          ls -la ~/.ollama/
        else
          echo "No model cache found - fresh download required"
        fi
    - name: Download AI model (if not cached)
      run: |
        if ! ollama list | grep -q "llama3.2:1b"; then
          echo "Downloading Llama 3.2 1B model..."
          ollama pull llama3.2:1b
        else
          echo "Model already available from cache"
        fi
        
    - name: Verify model availability
      run: |
        echo "Available models:"
        ollama list
        echo "Model download complete"
    - name: Execute AI query
      run: |
        echo "Processing AI prompt..."
        RESPONSE=$(ollama run llama3.2:1b "Explain what DevOps means in one sentence, focusing on automation and collaboration.")
        echo "AI Response:"
        echo "$RESPONSE"
    - name: Save AI response
      run: |
        mkdir -p outputs
        TIMESTAMP=$(date '+%Y-%m-%d_%H-%M-%S')
        ollama run llama3.2:1b "Explain what DevOps means in one sentence, focusing on automation and collaboration." > "outputs/ai-response-$TIMESTAMP.txt"
        echo "Response saved to outputs/ai-response-$TIMESTAMP.txt"
        cat "outputs/ai-response-$TIMESTAMP.txt"
    - name: Upload AI responses
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ai-responses-${{ github.run_number }}
        path: outputs/
        retention-days: 30 
    - name: Advanced AI analysis
      run: |
        PROMPT="Analyze this GitHub Actions workflow and suggest three specific improvements for production use. Focus on security, performance, and maintainability."
        echo "Advanced prompt: $PROMPT"
        RESPONSE=$(ollama run llama3.2:1b "$PROMPT")
        echo "AI Analysis:"
        echo "$RESPONSE"
        echo "$RESPONSE" > "outputs/workflow-analysis-$(date '+%Y-%m-%d_%H-%M-%S').txt"
    - name: Setup Python testing environment
      run: |
        python -m pip install --upgrade pip
        pip install pytest
    - name: Create AI workflow tests
      run: |
        cat > test_ai_workflow.py << 'EOF'
        import subprocess
        import pytest
        import os

        def test_ollama_service_health():
            """Verify Ollama service is responsive"""
            result = subprocess.run(['ollama', '--version'], capture_output=True, text=True)
            assert result.returncode == 0
            assert 'ollama version' in result.stdout.lower()
        def test_model_availability():
            """Verify required model is available"""
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
            assert result.returncode == 0
            assert 'llama3.2:1b' in result.stdout
        def test_basic_ai_functionality():
            """Test basic AI query and response"""
            result = subprocess.run([
                'ollama', 'run', 'llama3.2:1b', 
                'Respond with exactly: TEST_PASSED'
            ], capture_output=True, text=True, timeout=30)
            
            assert result.returncode == 0
            assert 'TEST_PASSED' in result.stdout

        def test_cache_directory_exists():
            """Verify cache directory was created"""
            cache_dir = os.path.expanduser('~/.ollama')
            assert os.path.exists(cache_dir)
        EOF
    - name: Run automated tests
      run: |
        python -m pytest test_ai_workflow.py -v
    - name: Performance benchmarking
      run: |
        echo "=== Workflow Performance Report ===" > performance-report.txt
        echo "Timestamp: $(date)" >> performance-report.txt
        echo "Workflow Run: ${{ github.run_number }}" >> performance-report.txt
        
        # Time a sample AI query
        echo "Testing AI response time..." >> performance-report.txt
        start_time=$(date +%s)
        ollama run llama3.2:1b "What is continuous integration?" > ai_response.txt
        end_time=$(date +%s)
        response_time=$((end_time - start_time))
        
        echo "AI Response Time: ${response_time} seconds" >> performance-report.txt
        echo "Response Content Length: $(wc -w < ai_response.txt) words" >> performance-report.txt
        
        cat performance-report.txt
    - name: Capture test results for AI analysis
      if: always()
      run: |
        # Create structured test output
        mkdir -p analysis
        
        # Re-run tests with detailed output for AI analysis
        python -m pytest test_ai_workflow.py -v --tb=short > analysis/test_output.txt 2>&1 || true
        
        # Check if any tests failed
        if grep -q "FAILED" analysis/test_output.txt; then
          echo "TEST_FAILURES=true" >> $GITHUB_ENV
          echo "Found test failures - preparing for AI analysis"
        else
          echo "TEST_FAILURES=false" >> $GITHUB_ENV
          echo "All tests passed - preparing success analysis"
        fi
        
        cat analysis/test_output.txt
    - name: Generate comprehensive test metrics
      if: always()
      run: |
        echo "ðŸ“Š Generating test metrics..."
        
        # Count test results
        TOTAL_TESTS=$(grep -c "test_" test_ai_workflow.py || echo "0")
        PASSED_TESTS=$(grep -c "PASSED" analysis/test_output.txt || echo "0")
        FAILED_TESTS=$(grep -c "FAILED" analysis/test_output.txt || echo "0")
        EXECUTION_TIME=$(grep "===.*seconds" analysis/test_output.txt | tail -1 || echo "Not available")
        
        # Create metrics summary
        cat > analysis/test_metrics.txt << EOF
        Test Execution Summary:
        - Total Tests: $TOTAL_TESTS
        - Passed: $PASSED_TESTS  
        - Failed: $FAILED_TESTS
        - Success Rate: $(( PASSED_TESTS * 100 / (TOTAL_TESTS > 0 ? TOTAL_TESTS : 1) ))%
        - Execution Time: $EXECUTION_TIME
        - Workflow Run: ${{ github.run_number }}
        - Timestamp: $(date)
        - Cache Performance: Previous model cache $([ -d ~/.ollama ] && echo "hit" || echo "miss")
        EOF
        
        cat analysis/test_metrics.txt
